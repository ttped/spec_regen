"""
yolo_asset_extractor.py - Extract figures and tables from document pages using DocLayout-YOLO.

This module:
1. Loads page images from docs_images/ (generated by prepare_images.py)
2. Runs DocLayout-YOLO to detect figures and tables
3. Crops detected regions and saves them as images
4. Generates metadata JSON files compatible with asset_processor.py

The output format matches what asset_processor.py expects, allowing seamless
integration into the existing pipeline without manual figure extraction.

Usage:
    python yolo_asset_extractor.py                    # Process all documents
    python yolo_asset_extractor.py --doc MyDocument   # Process specific document
    python yolo_asset_extractor.py --conf 0.5         # Custom confidence threshold
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict

from doclayout_yolo import YOLOv10
from PIL import Image

# =============================================================================
# CONFIGURATION
# =============================================================================

# HuggingFace model for pre-trained weights
HUGGINGFACE_MODEL = "juliozhao/DocLayout-YOLO-DocStructBench"

# DocLayout-YOLO class mapping (from DocStructBench)
YOLO_CLASSES = {
    0: "title",
    1: "plain_text",
    2: "abandon",
    3: "figure",
    4: "figure_caption",
    5: "table",
    6: "table_caption",
    7: "table_footnote",
    8: "isolate_formula",
    9: "formula_caption",
}

# Classes we want to extract as assets
ASSET_CLASSES = {"figure", "table", "isolate_formula"}

# Caption classes that validate assets
CAPTION_CLASSES = {"figure_caption", "table_caption", "formula_caption"}

# Mapping from asset type to its caption type
ASSET_TO_CAPTION = {
    "figure": "figure_caption",
    "table": "table_caption",
    "isolate_formula": "formula_caption",
}


@dataclass
class Detection:
    """Represents any YOLO detection (asset or caption)."""
    class_name: str
    bbox_pixels: Tuple[int, int, int, int]  # (x1, y1, x2, y2) in pixels
    confidence: float
    class_id: int


@dataclass
class DetectedAsset:
    """Represents a detected figure or table."""
    asset_type: str          # "figure" or "table"
    page_number: int         # 1-indexed page number
    bbox_pixels: Tuple[int, int, int, int]  # (x1, y1, x2, y2) in pixels
    confidence: float
    class_id: int
    doc_stem: str
    image_width: int
    image_height: int
    has_caption: bool = False        # Whether a matching caption was detected
    caption_bbox: Optional[Tuple[int, int, int, int]] = None  # Caption bbox if found
    validated_by: str = "none"       # "caption", "ocr_keyword", or "none"


def get_paths():
    """Calculate paths relative to this script file."""
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent if script_dir.name != "docs" else script_dir
    
    return {
        'images_dir': project_root / "docs_images",
        'exports_dir': project_root / "yolo_exports",  # Output directory for extracted assets
        'raw_ocr_dir': project_root / "iris_ocr" / "CM_Spec_OCR_and_figtab_output" / "raw_data_advanced",
    }


def load_model(local_path: str = None) -> 'YOLOv10':
    """Load DocLayout-YOLO model with default local path."""
    # Default to the specific model one folder up if no path provided
    if local_path is None:
        local_path = str(Path(__file__).resolve().parent.parent / "doclayout_yolo_docstructbench_imgsz1024.pt")
    
    if local_path and Path(local_path).exists():
        print(f"  Loading local model: {local_path}")
        return YOLOv10(local_path)
    else:
        print(f"  Loading pre-trained model from HuggingFace: {HUGGINGFACE_MODEL}")
        return YOLOv10.from_pretrained(HUGGINGFACE_MODEL)


def parse_image_filename(filename: str) -> Optional[Tuple[str, int]]:
    """
    Parse a page image filename to extract document stem and page number.
    
    Expected format: {doc_stem}_page{NNN}.jpg
    Example: "MyDocument_page001.jpg" -> ("MyDocument", 1)
    
    Returns:
        Tuple of (doc_stem, page_number) or None if parsing fails
    """
    import re
    
    # Match pattern: anything_pageNNN.jpg (or .jpeg, .png, etc.)
    match = re.match(r'^(.+)_page(\d+)\.[a-zA-Z]+$', filename)
    if match:
        doc_stem = match.group(1)
        page_num = int(match.group(2))
        return (doc_stem, page_num)
    return None


def group_images_by_document(images_dir: Path) -> Dict[str, List[Tuple[Path, int]]]:
    """
    Group page images by their document stem.
    
    Returns:
        Dict mapping doc_stem -> list of (image_path, page_number) tuples, sorted by page
    """
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    docs = defaultdict(list)
    
    if not images_dir.exists():
        print(f"[Error] Images directory not found: {images_dir}")
        return {}
    
    for img_path in images_dir.iterdir():
        if img_path.suffix.lower() not in image_extensions:
            continue
        
        parsed = parse_image_filename(img_path.name)
        if parsed:
            doc_stem, page_num = parsed
            docs[doc_stem].append((img_path, page_num))
    
    # Sort each document's pages by page number
    for doc_stem in docs:
        docs[doc_stem].sort(key=lambda x: x[1])
    
    return dict(docs)


def calculate_bbox_proximity(
    asset_bbox: Tuple[int, int, int, int],
    caption_bbox: Tuple[int, int, int, int],
    image_height: int
) -> Tuple[bool, float]:
    """
    Check if a caption bbox is near an asset bbox.
    
    Captions are typically directly above or below the asset.
    Returns (is_nearby, distance_ratio) where distance_ratio is relative to image height.
    """
    ax1, ay1, ax2, ay2 = asset_bbox
    cx1, cy1, cx2, cy2 = caption_bbox
    
    # Check horizontal overlap - caption should overlap horizontally with asset
    horizontal_overlap = min(ax2, cx2) - max(ax1, cx1)
    asset_width = ax2 - ax1
    
    if horizontal_overlap < asset_width * 0.3:  # At least 30% horizontal overlap
        return False, float('inf')
    
    # Calculate vertical distance
    # Caption above asset
    if cy2 <= ay1:
        vertical_distance = ay1 - cy2
    # Caption below asset
    elif cy1 >= ay2:
        vertical_distance = cy1 - ay2
    # Overlapping vertically (caption inside or overlapping asset)
    else:
        vertical_distance = 0
    
    # Normalize by image height
    distance_ratio = vertical_distance / image_height if image_height > 0 else float('inf')
    
    # Caption should be within 10% of image height from the asset
    is_nearby = distance_ratio < 0.10
    
    return is_nearby, distance_ratio


def find_matching_caption(
    asset: Detection,
    captions: List[Detection],
    asset_type: str,
    image_height: int
) -> Optional[Detection]:
    """
    Find a caption that matches the given asset.
    
    Args:
        asset: The figure or table detection
        captions: List of all caption detections on the page
        asset_type: "figure" or "table"
        image_height: Height of the page image
    
    Returns:
        The matching caption Detection, or None if not found
    """
    expected_caption_type = ASSET_TO_CAPTION.get(asset_type)
    if not expected_caption_type:
        return None
    
    best_caption = None
    best_distance = float('inf')
    
    for caption in captions:
        if caption.class_name != expected_caption_type:
            continue
        
        is_nearby, distance = calculate_bbox_proximity(
            asset.bbox_pixels, caption.bbox_pixels, image_height
        )
        
        if is_nearby and distance < best_distance:
            best_distance = distance
            best_caption = caption
    
    return best_caption


def check_ocr_for_keywords(
    raw_ocr_dir: str,
    doc_stem: str,
    page_number: int,
    asset_type: str
) -> bool:
    """
    Check if the OCR text for a page contains figure/table keywords.
    
    This is a fallback validation when no caption is detected by YOLO.
    Requires a numbered reference like "Figure 1", "Table 2.1", "Fig. 3" etc.
    
    Args:
        raw_ocr_dir: Directory containing raw OCR JSON files
        doc_stem: Document stem name
        page_number: Page number to check
        asset_type: "figure" or "table"
    
    Returns:
        True if relevant keywords are found on the page
    """
    import re
    
    if not raw_ocr_dir:
        return False
    
    ocr_path = os.path.join(raw_ocr_dir, f"{doc_stem}.json")
    if not os.path.exists(ocr_path):
        return False
    
    try:
        with open(ocr_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (json.JSONDecodeError, IOError):
        return False
    
    # Handle dict structure (keyed by page number)
    page_key = str(page_number)
    page_data = data.get(page_key)
    
    if not page_data:
        # Try integer key
        page_data = data.get(page_number)
    
    if not page_data:
        return False
    
    # Extract text from page
    page_dict = page_data.get('page_dict', page_data)
    text_list = page_dict.get('text', [])
    
    if not text_list:
        return False
    
    # Join all text and search for keywords
    page_text = ' '.join(str(t) for t in text_list).lower()
    
    # Require numbered references to reduce false positives
    # Matches: "figure 1", "figure 2.1", "fig. 3", "fig 4", "table 1", etc.
    if asset_type == "figure":
        pattern = r'\b(figure|fig\.?)\s*\d'
    elif asset_type == "table":
        pattern = r'\b(table|tab\.?)\s*\d'
    elif asset_type == "isolate_formula":
        # Equations/formulas: "equation 1", "eq. 2", "formula 3"
        pattern = r'\b(equation|eq\.?|formula)\s*\d'
    else:
        return False
    
    return bool(re.search(pattern, page_text))


def run_detection_on_image(
    model: 'YOLOv10',
    image_path: Path,
    doc_stem: str,
    page_number: int,
    confidence_threshold: float = 0.25,
    device: str = 'cpu',
    raw_ocr_dir: str = None
) -> List[DetectedAsset]:
    """
    Run YOLO detection on a single page image.
    
    Assets are validated by requiring either:
    1. A matching caption detected by YOLO (figure_caption/table_caption)
    2. Relevant keywords ("Figure", "Table") found in OCR text for the page
    
    Args:
        model: The YOLO model
        image_path: Path to the page image
        doc_stem: Document stem name
        page_number: Page number (1-indexed)
        confidence_threshold: Minimum confidence for detections
        device: Device to run inference on
        raw_ocr_dir: Directory containing raw OCR files (for keyword fallback)
    
    Returns:
        List of validated DetectedAsset objects for figures and tables
    """
    validated_assets = []
    
    # Run prediction
    results = model.predict(
        str(image_path),
        imgsz=1024,
        conf=confidence_threshold,
        save=False,
        device=device,
        verbose=False
    )
    
    if not results or len(results) == 0:
        return validated_assets
    
    result = results[0]
    
    # Get image dimensions
    img_height, img_width = result.orig_shape
    
    # Collect ALL detections first (assets and captions)
    all_detections: List[Detection] = []
    for box in result.boxes:
        class_id = int(box.cls)
        class_name = YOLO_CLASSES.get(class_id, "unknown")
        confidence = float(box.conf)
        x1, y1, x2, y2 = box.xyxy[0].tolist()
        
        detection = Detection(
            class_name=class_name,
            bbox_pixels=(int(x1), int(y1), int(x2), int(y2)),
            confidence=confidence,
            class_id=class_id
        )
        all_detections.append(detection)
    
    # Separate assets and captions
    asset_detections = [d for d in all_detections if d.class_name in ASSET_CLASSES]
    caption_detections = [d for d in all_detections if d.class_name in CAPTION_CLASSES]
    
    # Process each asset detection
    for detection in asset_detections:
        asset_class = detection.class_name  # "figure", "table", or "isolate_formula"
        if asset_class == "figure":
            asset_type = "fig"
        elif asset_class == "table":
            asset_type = "tab"
        elif asset_class == "isolate_formula":
            asset_type = "eq"
        else:
            continue
        
        # Try to find a matching caption
        matching_caption = find_matching_caption(
            detection, caption_detections, asset_class, img_height
        )
        
        has_caption = matching_caption is not None
        caption_bbox = matching_caption.bbox_pixels if matching_caption else None
        validated_by = "caption" if has_caption else "none"
        
        # If no caption found, try OCR keyword fallback
        if not has_caption and raw_ocr_dir:
            has_keyword = check_ocr_for_keywords(
                raw_ocr_dir, doc_stem, page_number, asset_class
            )
            if has_keyword:
                validated_by = "ocr_keyword"
        
        # Formulas are accepted without validation - they commonly appear
        # standalone without captions or keyword references
        if not has_caption and asset_class == "isolate_formula":
            validated_by = "standalone"
        
        # Skip unvalidated assets (figures and tables still require validation)
        if validated_by == "none":
            print(f"      [Rejected] {asset_class} on page {page_number} - no caption or keyword found")
            continue
        
        asset = DetectedAsset(
            asset_type=asset_type,
            page_number=page_number,
            bbox_pixels=detection.bbox_pixels,
            confidence=detection.confidence,
            class_id=detection.class_id,
            doc_stem=doc_stem,
            image_width=img_width,
            image_height=img_height,
            has_caption=has_caption,
            caption_bbox=caption_bbox,
            validated_by=validated_by
        )
        validated_assets.append(asset)
    
    return validated_assets


def crop_and_save_asset(
    source_image_path: Path,
    asset: DetectedAsset,
    output_dir: Path,
    asset_index: int,
    padding: int = 5
) -> Tuple[str, Path]:
    """
    Crop the detected region from the source image and save it.
    
    Args:
        source_image_path: Path to the full page image
        asset: The detected asset with bounding box
        output_dir: Directory to save cropped images
        asset_index: Index for unique naming
        padding: Pixels to add around the bounding box (to avoid cutting off edges)
    
    Returns:
        Tuple of (image_filename, full_path)
    """
    # Generate filename: {doc_stem}_{type}_{page}_{index}.jpg
    # Example: MyDocument_fig_p003_001.jpg
    filename = f"{asset.doc_stem}_{asset.asset_type}_p{asset.page_number:03d}_{asset_index:03d}.jpg"
    output_path = output_dir / filename
    
    # Open source image and crop
    with Image.open(source_image_path) as img:
        x1, y1, x2, y2 = asset.bbox_pixels
        
        # Add padding (but don't exceed image bounds)
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(img.width, x2 + padding)
        y2 = min(img.height, y2 + padding)
        
        # Crop and save
        cropped = img.crop((x1, y1, x2, y2))
        cropped.save(output_path, "JPEG", quality=95)
    
    return filename, output_path


def crop_and_save_caption(
    source_image_path: Path,
    asset: DetectedAsset,
    output_dir: Path,
    asset_index: int,
    padding: int = 5
) -> Tuple[str, Path]:
    """
    Crop the caption region associated with a detected asset and save it.
    
    Naming follows: {doc_stem}_{type}_p{page}_{index}_caption.jpg
    Example: MyDocument_fig_p003_001_caption.jpg
    
    Args:
        source_image_path: Path to the full page image
        asset: The detected asset (must have caption_bbox set)
        output_dir: Directory to save cropped caption images
        asset_index: Index matching the parent asset
        padding: Pixels to add around the bounding box
    
    Returns:
        Tuple of (caption_filename, full_path)
    """
    filename = f"{asset.doc_stem}_{asset.asset_type}_p{asset.page_number:03d}_{asset_index:03d}_caption.jpg"
    output_path = output_dir / filename
    
    with Image.open(source_image_path) as img:
        x1, y1, x2, y2 = asset.caption_bbox
        
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(img.width, x2 + padding)
        y2 = min(img.height, y2 + padding)
        
        cropped = img.crop((x1, y1, x2, y2))
        cropped.save(output_path, "JPEG", quality=95)
    
    return filename, output_path


def create_asset_metadata(
    asset: DetectedAsset,
    image_filename: str,
    output_dpi: int = 200,
    caption_image_filename: str = None
) -> Dict:
    """
    Create metadata JSON for an asset in the format expected by asset_processor.py.
    
    This matches the structure of manually-exported assets so the downstream
    pipeline works without modification.
    """
    x1, y1, x2, y2 = asset.bbox_pixels
    width = x2 - x1
    height = y2 - y1
    
    # Generate a unique asset_id
    asset_id = f"{asset.doc_stem}_{asset.asset_type}_p{asset.page_number}_{x1}_{y1}"
    
    metadata = {
        "asset_id": asset_id,
        "asset_type": asset.asset_type,  # "fig", "tab", or "eq"
        "page": asset.page_number,
        "bbox": {
            "pixels": [x1, y1, width, height],  # [x, y, width, height] format
            "page_width_px": asset.image_width,
            "page_height_px": asset.image_height,
        },
        "export": {
            "image_file": image_filename,
            "caption_image_file": caption_image_filename,
            "dpi": output_dpi,
            "format": "JPEG",
        },
        "detection": {
            "method": "DocLayout-YOLO",
            "model": HUGGINGFACE_MODEL,
            "confidence": round(asset.confidence, 4),
            "class_id": asset.class_id,
            "class_name": YOLO_CLASSES.get(asset.class_id, "unknown"),
            "validated_by": asset.validated_by,
            "has_caption": asset.has_caption,
        }
    }
    
    # Include caption bbox if available
    if asset.caption_bbox:
        cx1, cy1, cx2, cy2 = asset.caption_bbox
        metadata["detection"]["caption_bbox"] = {
            "pixels": [cx1, cy1, cx2 - cx1, cy2 - cy1],
        }
    
    return metadata


def process_document(
    model: 'YOLOv10',
    doc_stem: str,
    page_images: List[Tuple[Path, int]],
    output_dir: Path,
    confidence_threshold: float = 0.25,
    device: str = 'cpu',
    raw_ocr_dir: str = None
) -> List[Dict]:
    """
    Process all pages of a document and extract validated assets.
    
    Args:
        model: The YOLO model
        doc_stem: Document stem name
        page_images: List of (image_path, page_number) tuples
        output_dir: Directory for output files
        confidence_threshold: Minimum confidence for detections
        device: Device to run inference on
        raw_ocr_dir: Directory containing raw OCR files (for keyword fallback)
    
    Returns:
        List of metadata dictionaries for all validated assets
    """
    # Create document-specific output directory
    doc_output_dir = output_dir / doc_stem
    doc_output_dir.mkdir(parents=True, exist_ok=True)
    
    all_assets_metadata = []
    asset_counters = {"fig": 0, "tab": 0, "eq": 0}
    validation_stats = {"caption": 0, "ocr_keyword": 0, "standalone": 0}
    
    print(f"  Processing {len(page_images)} pages...")
    
    for image_path, page_number in page_images:
        # Run detection
        detected_assets = run_detection_on_image(
            model=model,
            image_path=image_path,
            doc_stem=doc_stem,
            page_number=page_number,
            confidence_threshold=confidence_threshold,
            device=device,
            raw_ocr_dir=raw_ocr_dir
        )
        
        if detected_assets:
            print(f"    Page {page_number}: Found {len(detected_assets)} validated assets")
        
        # Process each detected asset
        for asset in detected_assets:
            asset_counters[asset.asset_type] += 1
            asset_index = asset_counters[asset.asset_type]
            validation_stats[asset.validated_by] += 1
            
            # Crop and save the asset image
            image_filename, _ = crop_and_save_asset(
                source_image_path=image_path,
                asset=asset,
                output_dir=doc_output_dir,
                asset_index=asset_index
            )
            
            # Crop and save the caption image if one was detected
            caption_image_filename = None
            if asset.has_caption and asset.caption_bbox:
                caption_image_filename, _ = crop_and_save_caption(
                    source_image_path=image_path,
                    asset=asset,
                    output_dir=doc_output_dir,
                    asset_index=asset_index
                )
            
            # Create metadata
            metadata = create_asset_metadata(asset, image_filename, caption_image_filename=caption_image_filename)
            all_assets_metadata.append(metadata)
            
            # Save individual metadata JSON (for compatibility with asset_processor)
            json_filename = image_filename.replace('.jpg', '.json')
            json_path = doc_output_dir / json_filename
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)
    
    # Print validation stats
    print(f"  Validation: {validation_stats['caption']} by caption, {validation_stats['ocr_keyword']} by OCR keyword, {validation_stats['standalone']} standalone")
    
    return all_assets_metadata


def run_yolo_extraction(
    images_dir: Optional[Path] = None,
    output_dir: Optional[Path] = None,
    doc_filter: Optional[str] = None,
    confidence_threshold: float = 0.25,
    device: str = 'cpu',
    model_path: Optional[str] = None,
    raw_ocr_dir: Optional[str] = None
) -> Dict[str, List[Dict]]:
    """
    Main function to run YOLO extraction on all documents (or a specific one).
    
    Assets are validated by requiring either a detected caption or OCR keywords.
    
    Args:
        images_dir: Directory containing page images (defaults to docs_images/)
        output_dir: Directory for output (defaults to yolo_exports/)
        doc_filter: If provided, only process this document stem
        confidence_threshold: Minimum confidence for detections
        device: 'cpu', 'cuda:0', or 'mps' for Apple Silicon
        model_path: Optional path to local model weights
        raw_ocr_dir: Directory containing raw OCR files (auto-inferred if not provided)
    
    Returns:
        Dict mapping doc_stem -> list of asset metadata
    """
    # Get default paths if not provided
    paths = get_paths()
    images_dir = Path(images_dir) if images_dir else paths['images_dir']
    output_dir = Path(output_dir) if output_dir else paths['exports_dir']
    
    # Auto-infer raw_ocr_dir if not provided
    if raw_ocr_dir is None:
        raw_ocr_dir = str(paths['raw_ocr_dir'])
    
    # Verify raw_ocr_dir exists
    if not os.path.isdir(raw_ocr_dir):
        print(f"  [WARNING] Raw OCR directory not found: {raw_ocr_dir}")
        print(f"  [WARNING] OCR keyword validation will not work - only caption detection will be used")
        raw_ocr_dir = None
    
    print("=" * 60)
    print("YOLO ASSET EXTRACTION")
    print("=" * 60)
    print(f"  Images directory: {images_dir}")
    print(f"  Output directory: {output_dir}")
    print(f"  Confidence threshold: {confidence_threshold}")
    print(f"  Device: {device}")
    print(f"  Raw OCR directory: {raw_ocr_dir or '(not found)'}")
    print()
    
    # Group images by document
    docs = group_images_by_document(images_dir)
    
    if not docs:
        print("[Error] No page images found!")
        return {}
    
    # Filter to specific document if requested
    if doc_filter:
        if doc_filter in docs:
            docs = {doc_filter: docs[doc_filter]}
        else:
            print(f"[Error] Document '{doc_filter}' not found in images directory.")
            print(f"  Available documents: {list(docs.keys())}")
            return {}
    
    print(f"  Found {len(docs)} document(s) to process")
    print()
    
    # Load model once
    print("Loading YOLO model...")
    model = load_model(model_path)
    if model is None:
        return {}
    print()
    
    # Process each document
    output_dir.mkdir(parents=True, exist_ok=True)
    results = {}
    
    for doc_stem, page_images in docs.items():
        print(f"[{doc_stem}]")
        
        assets = process_document(
            model=model,
            doc_stem=doc_stem,
            page_images=page_images,
            output_dir=output_dir,
            confidence_threshold=confidence_threshold,
            device=device,
            raw_ocr_dir=raw_ocr_dir
        )
        
        results[doc_stem] = assets
        
        # Summary for this document
        fig_count = sum(1 for a in assets if a['asset_type'] == 'fig')
        tab_count = sum(1 for a in assets if a['asset_type'] == 'tab')
        eq_count = sum(1 for a in assets if a['asset_type'] == 'eq')
        print(f"  Extracted: {fig_count} figures, {tab_count} tables, {eq_count} equations")
        print()
    
    # Overall summary
    print("=" * 60)
    print("EXTRACTION COMPLETE")
    print("=" * 60)
    total_figs = sum(sum(1 for a in assets if a['asset_type'] == 'fig') for assets in results.values())
    total_tabs = sum(sum(1 for a in assets if a['asset_type'] == 'tab') for assets in results.values())
    total_eqs = sum(sum(1 for a in assets if a['asset_type'] == 'eq') for assets in results.values())
    print(f"  Total figures: {total_figs}")
    print(f"  Total tables: {total_tabs}")
    print(f"  Total equations: {total_eqs}")
    print(f"  Output: {output_dir}")
    
    return results

# =============================================================================
# PIPELINE INTEGRATION HELPER
# =============================================================================

def get_yolo_exports_dir() -> Path:
    """
    Get the YOLO exports directory path.
    
    This is used by simple_pipeline.py to point asset_processor.py
    at the YOLO-generated assets instead of manual exports.
    """
    return get_paths()['exports_dir']


# =============================================================================
# CLI
# =============================================================================

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Extract figures and tables from document pages using DocLayout-YOLO"
    )
    parser.add_argument(
        '--images-dir', '-i',
        type=str,
        default=None,
        help="Directory containing page images (default: docs_images/)"
    )
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default=None,
        help="Output directory for extracted assets (default: yolo_exports/)"
    )
    parser.add_argument(
        '--doc', '-d',
        type=str,
        default=None,
        help="Process only this document stem"
    )
    parser.add_argument(
        '--conf', '-c',
        type=float,
        default=0.25,
        help="Confidence threshold for detections (default: 0.25)"
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cpu',
        help="Device: 'cpu', 'cuda:0', or 'mps' (default: cpu)"
    )
    parser.add_argument(
        '--model',
        type=str,
        default=None,
        help="Path to local model weights (optional)"
    )
    parser.add_argument(
        '--raw-ocr-dir',
        type=str,
        default=None,
        help="Directory containing raw OCR JSON files (auto-inferred if not provided)"
    )
    
    args = parser.parse_args()
    
    run_yolo_extraction(
        images_dir=Path(args.images_dir) if args.images_dir else None,
        output_dir=Path(args.output_dir) if args.output_dir else None,
        doc_filter=args.doc,
        confidence_threshold=args.conf,
        device=args.device,
        model_path=args.model,
        raw_ocr_dir=args.raw_ocr_dir
    )