import os
import json
import re
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from .utils import _extract_json_from_llm_string, call_llm, load_pages_from_json, save_results_to_json
except ImportError:
    from utils import _extract_json_from_llm_string, call_llm, load_pages_from_json, save_results_to_json

def _repair_json_with_llm(broken_json_string: str, original_prompt_rules: str, llm_config: Dict) -> str:
    """
    Attempts to repair a malformed JSON string by sending it back to the LLM.

    Args:
        broken_json_string: The string that failed to parse.
        original_prompt_rules: The rules from the original prompt to give the LLM context.
        llm_config: The LLM configuration.

    Returns:
        A potentially corrected JSON string.
    """
    print("  - [Warning] Malformed JSON detected. Attempting self-repair with LLM...")
    
    repair_prompt = f"""
You are a JSON repair expert. The following text was generated by an AI trying to follow a set of rules, but it resulted in a malformed JSON string that cannot be parsed.

Your task is to analyze the original rules and the broken text, correct any syntax errors (like missing commas, brackets, or unescaped quotes), and return ONLY the corrected, valid JSON object.

--- ORIGINAL RULES ---
{original_prompt_rules}
--- END ORIGINAL RULES ---

--- BROKEN JSON TEXT ---
{broken_json_string}
--- END BROKEN JSON TEXT ---

Please provide only the corrected, raw JSON string in your response.
"""
    
    repaired_response = call_llm(
        repair_prompt,
        llm_config['model_name'],
        llm_config['base_url'],
        llm_config['api_key'],
        llm_config['provider']
    )
    
    # Return the cleaned-up response from the repair attempt
    return _extract_json_from_llm_string(repaired_response)

def clean_page_with_llm(page_text: str, llm_config: Dict) -> str:
    """
    Uses an LLM to remove headers and footers. On failure, it now returns the
    original text to prevent data loss.
    """
    scrubbed_page_text = page_text.replace('"', '~~')

    prompt = f"""
You are an expert text-cleaning utility. Your task is to analyze the following document page text and remove all header and footer content.

Your goal is to isolate ONLY the core body of the text.

**CRITICAL OUTPUT FORMAT:** You MUST return a single JSON object with one key: "cleaned_text". The value of this key should be the cleaned text content.

**Example Output:**
{{
  "cleaned_text": "This is the main body content of the page, with headers and footers removed."
}}

**Page Text to Clean:**
---
{scrubbed_page_text}
---
"""
    llm_response = call_llm(prompt, llm_config['model_name'], llm_config['base_url'], llm_config['api_key'], llm_config['provider'])

    json_string = _extract_json_from_llm_string(llm_response)
    if not json_string:
        # DATA PRESERVATION: If LLM fails, return the original text.
        print("  - LLM cleaning failed to produce JSON, returning original text.")
        return page_text

    try:
        data = json.loads(json_string)
        # Return the cleaned text, which still has '~~' placeholders.
        return data.get("cleaned_text", page_text)
    except json.JSONDecodeError:
        # DATA PRESERVATION: If JSON is malformed, return the original text.
        print("  - LLM cleaning produced malformed JSON, returning original text.")
        return page_text

def clean_page_wrapper(args: Tuple[int, str, Dict]) -> Dict:
    """Wrapper to run the LLM cleaning function in parallel."""
    page_num, page_text, llm_config = args
    cleaned_text = clean_page_with_llm(page_text, llm_config)
    return {'page': page_num, 'cleaned_text': cleaned_text}

def clean_section_number(raw_number: str) -> str:
    """Cleans the raw section number string from the LLM."""
    if not raw_number: return ""
    match = re.search(r'(\d[\d\.]*)', raw_number)
    return match.group(1).strip().rstrip('.') if match else ""  

def extract_elements_from_page_chunk(page_chunk: str, llm_config: Dict) -> Dict:
    """
    LLM call to identify sections and unassigned text within a single chunk.
    This version has a much stricter prompt to prevent hallucinating sections
    from lists of documents or standards.
    """
    prompt_rules = """
1.  **Strict Section Definition:** A section is ONLY defined by a hierarchical number (e.g., "1.", "1.1", "2.1.3") followed by a title.
2.  **IGNORE Document Lists:** Text that looks like a list of standards, specifications, or military documents (e.g., "MIL-STD-785", "FED-STD-101C", "S-133-30001") are **NOT** section headers. You must ignore them when creating section objects.
3.  **IGNORE Figures and Tables:** Text like "Figure 1" or "Table 3" are **NOT** section headers.
4.  **Handle Empty Sections:** If a section header appears without any content following it in this chunk, you MUST create a section object with an empty string for its `content`.
5.  **Output Format:** Return a single JSON object with two keys: `sections` (a list of section objects) and `unassigned_text` (a string for any text that doesn't belong to a new section in this chunk).
"""

    prompt = f"""
You are a document structure analyst. Your task is to analyze the following text and identify structured elements based on a very strict set of rules.

**CRITICAL Rules:**
{prompt_rules}

Analyze the text below and follow these rules precisely:
---
{page_chunk}
---
"""
    llm_response = call_llm(prompt, llm_config['model_name'], llm_config['base_url'], llm_config['api_key'], llm_config['provider'])
    json_string = _extract_json_from_llm_string(llm_response)

    if not json_string:
        return {"sections": [], "unassigned_text": page_chunk}
    
    try:
        data = json.loads(json_string)
        
        if isinstance(data, dict):
            if "sections" not in data: data["sections"] = []
            if "unassigned_text" not in data: data["unassigned_text"] = ""
            return data
        elif isinstance(data, list):
            return {"sections": data, "unassigned_text": ""}
        else:
            return {"sections": [], "unassigned_text": page_chunk}

    except json.JSONDecodeError:
        repaired_json_string = _repair_json_with_llm(json_string, prompt_rules, llm_config)
        
        if not repaired_json_string:
            return {"sections": [], "unassigned_text": page_chunk}
            
        try:
            data = json.loads(repaired_json_string)
            print("  - [Success] LLM self-repair was successful.")
            
            if isinstance(data, dict):
                if "sections" not in data: data["sections"] = []
                if "unassigned_text" not in data: data["unassigned_text"] = ""
                return data
            elif isinstance(data, list):
                return {"sections": data, "unassigned_text": ""}
            else:
                return {"sections": [], "unassigned_text": page_chunk}

        except json.JSONDecodeError:
            print("  - [Error] LLM self-repair failed. Preserving chunk as unassigned text.")
            return {"sections": [], "unassigned_text": page_chunk}

def load_figures_data(base_exports_path: str, document_stem: str) -> List[Dict]:
    """Loads figure/table metadata, adding a chunk_index for sorting."""
    all_assets = []
    asset_dir = os.path.join(base_exports_path, document_stem)
    if not os.path.isdir(asset_dir): return []
    for filename in os.listdir(asset_dir):
        if filename.endswith(".json"):
            with open(os.path.join(asset_dir, filename), 'r', encoding='utf-8') as f:
                meta = json.load(f)
            meta['type'] = 'figure' if meta.get("asset_type") == "fig" else 'table'
            # Assign a high chunk_index to place assets at the end of their page during sorting.
            meta['chunk_index'] = 9999
            all_assets.append(meta)
    return all_assets

def process_page_context_in_chunks(args: Tuple[int, str, Dict, int, int]) -> List[Dict]:
    """
    A worker function for parallel execution. It takes a page's full text context,
    breaks it into overlapping chunks, and calls the LLM on each chunk.
    """
    page_num, full_context, llm_config, chunk_size, overlap = args
    
    if not full_context:
        return []

    chunk_results = []
    start = 0
    chunk_index = 0
    while start < len(full_context):
        end = start + chunk_size
        chunk_text = full_context[start:end]
        
        # Call the LLM for the current chunk
        extracted_data = extract_elements_from_page_chunk(chunk_text, llm_config)
        
        # Store the result with its page and chunk index for ordering
        chunk_results.append({
            'page': page_num,
            'chunk_index': chunk_index,
            'data': extracted_data
        })
        
        if end >= len(full_context):
            break
        
        start += chunk_size - overlap
        chunk_index += 1
        
    return chunk_results

def extract_elements_wrapper(args: Tuple[int, str, Dict]) -> Dict:
    """
    Wrapper to call extract_elements_from_page_chunk in parallel.
    Returns a dictionary containing the original page number and the extracted data.
    """
    page_num, page_text, llm_config = args
    extracted_data = extract_elements_from_page_chunk(page_text, llm_config)
    return {'page': page_num, 'data': extracted_data}


def run_organization_on_file(input_path: str, output_path: str, llm_config: Dict, figures_base_path: str, doc_stem: str, max_workers: int = 4, chunk_size: int = 1200, overlap: int = 150, **kwargs):
    """
    Performs data extraction, normalizes the output schema, and integrates figure/table metadata.
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        all_pages_from_file = json.load(f)

    content_pages = sorted(
        [p for p in all_pages_from_file if json.loads(p.get('classification', '{}')).get('subject') == "Contents"],
        key=lambda p: int(p['page'])
    )
    if not content_pages:
        save_results_to_json([], output_path)
        return

    page_text_map = {str(p['page']): p.get('text', '') for p in content_pages}
    
    pre_clean_pages = kwargs.get("pre_clean_pages", False)
    if pre_clean_pages:
        print(f"Pre-cleaning {len(content_pages)} pages with LLM using {max_workers} workers...")
        cleaning_tasks = [(p['page'], page_text_map[str(p['page'])], llm_config) for p in content_pages]
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for future in as_completed([executor.submit(clean_page_wrapper, task) for task in cleaning_tasks]):
                try:
                    result = future.result()
                    page_text_map[str(result['page'])] = result['cleaned_text'].replace('~~', '"')
                except Exception as exc:
                    task_info = future.result()
                    page_num = task_info[0]
                    print(f"  - Page {page_num} cleaning generated an exception: {exc}. Using original text.")
    else:
        print("Pre-cleaning step skipped.")

    analysis_tasks = []
    for i, page_data in enumerate(content_pages):
        current_page_num = page_data['page']
        current_text = page_text_map.get(str(current_page_num), '')
        prev_page_num_str = str(content_pages[i-1]['page']) if i > 0 else None
        next_page_num_str = str(content_pages[i+1]['page']) if i < len(content_pages) - 1 else None
        prev_text_tail = page_text_map.get(prev_page_num_str, '')[-overlap:] if prev_page_num_str else ''
        next_text_head = page_text_map.get(next_page_num_str, '')[:overlap] if next_page_num_str else ''
        full_context = prev_text_tail + current_text + next_text_head
        analysis_tasks.append((current_page_num, full_context, llm_config, chunk_size, overlap))

    print(f"\nAnalyzing {len(analysis_tasks)} page contexts in parallel...")
    all_processed_chunks = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for future in as_completed([executor.submit(process_page_context_in_chunks, task) for task in analysis_tasks]):
            try:
                all_processed_chunks.extend(future.result())
            except Exception as exc:
                print(f"  - A page analysis task generated an exception: {exc}")

    all_processed_chunks.sort(key=lambda x: (x['page'], x['chunk_index']))
    
    raw_elements = []
    for chunk in all_processed_chunks:
        extracted_data = chunk['data']
        unassigned_text = extracted_data.get('unassigned_text', '').strip()
        sections_from_chunk = extracted_data.get('sections', [])

        if unassigned_text:
            raw_elements.append({
                'type': 'unassigned_text_block',
                'content': unassigned_text,
                'page': chunk['page'],
                'chunk_index': chunk['chunk_index'],
            })

        # --- START: NORMALIZE LLM OUTPUT ---
        # This block ensures the output schema is consistent for the docx_writer.
        for i, section in enumerate(sections_from_chunk):
            header_text = section.get('header', '').strip()
            
            # Default values
            sec_num = section.get('section_number', '')
            sec_topic = section.get('topic', '')

            # If the LLM provided a 'header', parse it.
            if header_text:
                # Regex to capture a leading number (e.g., "1.", "3.2.1") and the text that follows.
                match = re.match(r'^\s*(\d[\d\.]*)\s*(.*)', header_text)
                if match:
                    sec_num = match.group(1).strip().rstrip('.')
                    sec_topic = match.group(2).strip()
                else:
                    # If no number is found, treat the whole header as the topic.
                    sec_topic = header_text
            
            # Create a clean, standardized section object.
            normalized_section = {
                'type': 'section',
                'section_number': sec_num,
                'topic': sec_topic,
                'content': section.get('content', '').strip(),
                'page': chunk['page'],
                'chunk_index': chunk['chunk_index'],
                'sub_chunk_index': i
            }
            raw_elements.append(normalized_section)
        # --- END: NORMALIZE LLM OUTPUT ---

    # --- INTEGRATE FIGURES ---
    print("\nLoading and merging figure/table data...")
    all_assets = load_figures_data(figures_base_path, doc_stem)
    
    final_elements = raw_elements + all_assets
    final_elements.sort(key=lambda x: (x.get('page', 9999), x.get('chunk_index', 9999), x.get('sub_chunk_index', 0)))
    # --- END INTEGRATE FIGURES ---

    print(f"\nSaving {len(final_elements)} raw elements (including figures).")
    save_results_to_json(final_elements, output_path)
    print(f"Raw organized elements saved to {output_path}")
    
if __name__ == '__main__':
    llm_config = {
        "provider": "mission_assist",
        "model_name": "gemma3",
        "base_url": "http://devmissionassist.api.us.baesystems.com",
        "api_key": "aTOIT9hJM3DBYMQbEY"
    }
    
    doc_stem = "S-133-05737_U_SSDD_CUI_classified"
    input_file = os.path.join("..", "results", f"{doc_stem}_classified.json")
    output_file = os.path.join("..", "results", f"{doc_stem}_organized_raw.json")
    figures_path = os.path.join("..", "iris_ocr", "CM_Spec_OCR_and_figtab_output", "exports")


    run_organization_on_file(
        input_file, 
        output_file, 
        llm_config,
        figures_base_path=figures_path,
        doc_stem=doc_stem,
        max_workers=4,
        chunk_size=1000,
        overlap=250
    )
